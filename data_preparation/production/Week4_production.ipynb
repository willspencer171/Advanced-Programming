{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Initial Analysis\n",
    "\n",
    "The first stage is to clean the data and make sure it is fit for purpose. Examine the data careful to identify anomalies and then consider how your program can identify these and correct/delete or change. You will need to consider how you are going to handle erroneous or missing values. You should output a sample of the data that demonstrates how cleaning has changed the data.\n",
    "\n",
    "The next stage is to reshape the data as per any requirements of the brief (or your own scenario). Is all the data needed to provide the required results? Is any of it duplicated? Is there data across different sources that needs to be brought together? Again, output a sample to the console to demonstrate how this has changed the structure of the data.\n",
    "\n",
    "Finally develop and test a set of functions (or objects and methods) that applies the statistical analysis to the data set, outputting the results to the console.\n",
    "\n",
    "Capture the results of your data cleaning, shaping and functions with screenshots of the consol. There is no requirement at this stage for anything to be functioning in through the GUI. Make sure it is clear what your output is testing/demonstrating (output simple informative statements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymongo\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "class CleaningFunctions:\n",
    "    '''Class containing related data cleaning functions'''\n",
    "    @staticmethod\n",
    "    def _map_to_most_similar(values, counts, threshold=80):\n",
    "        \"\"\"\n",
    "        Map each value to the most frequent one among its close matches.\n",
    "        \"\"\"\n",
    "        result_mapping = {}\n",
    "        for value in values:\n",
    "            matches = process.extract(value, values, scorer=fuzz.ratio, limit=None)\n",
    "            # Filter matches by threshold\n",
    "            similar = [(match, counts[values.index(match)]) for match, score, _ in matches if score >= threshold]\n",
    "            if similar:\n",
    "                # Find the most frequent one\n",
    "                most_frequent = max(similar, key=lambda x: x[1])[0]\n",
    "                result_mapping[value] = most_frequent\n",
    "            else:\n",
    "                # No similar match; keep the value itself\n",
    "                result_mapping[value] = value\n",
    "        return result_mapping\n",
    "\n",
    "    @staticmethod\n",
    "    def tidy_col_values(column: pd.Series, whitespace_to_nan=True, strip_whitespace=True,\n",
    "                        normalise_case: str | None = 'title', fuzzy_threshold=80):\n",
    "        '''Function that normalises column values'''\n",
    "        cases = {'title': column.str.title,\n",
    "                'lower': column.str.lower,\n",
    "                'upper': column.str.upper}\n",
    "        \n",
    "        if whitespace_to_nan:\n",
    "            # Convert missed null values\n",
    "            column = column.replace([r'^\\s+$', '^$'], np.nan, regex=True)\n",
    "\n",
    "        if strip_whitespace:\n",
    "            # Strip whitespace from values\n",
    "            column = column.str.strip()\n",
    "\n",
    "        if normalise_case:\n",
    "            if normalise_case not in cases:\n",
    "                raise KeyError(f\"'{normalise_case}' not accepted value for normalise_case. Please choose from {list(cases.keys())}\")\n",
    "            \n",
    "            # Normalise case\n",
    "            column = cases[normalise_case]()\n",
    "\n",
    "        # NLP to match similar value names (e.g. Submission_state and Submission_status)\n",
    "        column = column.replace(CleaningFunctions._map_to_most_similar(column.unique().tolist(), \n",
    "                                                    column.value_counts().tolist(),\n",
    "                                                    threshold=fuzzy_threshold))\n",
    "\n",
    "        return column\n",
    "    \n",
    "    def reshaping(obj: pd.DataFrame | pd.Series, reshape: tuple[int, int] | None = None, transpose=False):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "client = pymongo.MongoClient('localhost', 27017)\n",
    "db = client['summative']\n",
    "\n",
    "act_log, comp_codes, user_log = [db[name] for name in db.list_collection_names()]\n",
    "act_log_df = pd.DataFrame(act_log.find({})).set_index('_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = act_log_df.copy()\n",
    "for col in act_log_df.columns[1:]:\n",
    "    temp_df[col] = CleaningFunctions.tidy_col_values(temp_df[col], normalise_case='lower', fuzzy_threshold=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
