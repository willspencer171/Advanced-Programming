{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refactoring the following functionality using parallel processing:\n",
    "\n",
    "```python\n",
    "for dataset in datasets:\n",
    "    mode_action = dataset.groupby('Component')['Action'].agg(lambda x: x.mode())\n",
    "\n",
    "    interactions_per_user = dataset.groupby(['Component', 'User_ID']).size()\\\n",
    "        .reset_index(name='Interaction_count')\n",
    "    mean_interactions_per_user = interactions_per_user.groupby('Component')['Interaction_count'].mean()\n",
    "\n",
    "    median_date_of_component_interaction = dataset.groupby('Component')['Date'].median()\n",
    "\n",
    "    results.append(pd.DataFrame({'Mode Action': mode_action,\n",
    "                            'Mean Interactions per User': mean_interactions_per_user,\n",
    "                            'Median Date of Interaction': median_date_of_component_interaction}).reset_index())\n",
    "    \n",
    "return results\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "from os import path\n",
    "\n",
    "%run pickling_db_funcs.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_df(df: pd.DataFrame, chunksize):\n",
    "    return [df.iloc[i:i+chunksize] for i in range(0, len(df), chunksize)]\n",
    "\n",
    "def json_chunk(chunk:pd.DataFrame, index):\n",
    "    chunk.to_json(path.join(DATA_FOLDER, 'json_conc', f'_{index}.json'), orient='records')\n",
    "\n",
    "def thread_save_to_json(df):\n",
    "    chunks = chunk_df(df, len(df) // 6)\n",
    "\n",
    "    with ThreadPoolExecutor() as exe:\n",
    "        futures = [exe.submit(json_chunk, chunk, i) for i, chunk in enumerate(chunks)]\n",
    "\n",
    "def normal_save_to_json(df):\n",
    "    df.to_json(path.join(DATA_FOLDER, 'json_conc', 'test.json'), orient='records')\n",
    "\n",
    "def benchmark(dataset, func, reps=20):\n",
    "    times = []\n",
    "    for _ in range(reps):\n",
    "        start_time = time.time()\n",
    "        func(dataset)\n",
    "        times.append(time.time() - start_time)\n",
    "    \n",
    "    avg_time = sum(times) / reps\n",
    "    print(f'{func.__name__} - Average Time: {avg_time:.4f} seconds over {reps} runs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread_save_to_json - Average Time: 1.3939 seconds over 50 runs\n",
      "normal_save_to_json - Average Time: 1.4492 seconds over 50 runs\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.DataFrame(read_pickle_backup('merged'))\n",
    "test_data = pd.concat([test_data] * 10)\n",
    "benchmark(test_data, thread_save_to_json, reps=50)\n",
    "benchmark(test_data, normal_save_to_json, reps=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
